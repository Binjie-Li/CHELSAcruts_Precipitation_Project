{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('USW00024233', 1, 5114), ('USC00458508', 2, 5114)]\n"
     ]
    }
   ],
   "source": [
    "def process_data(filepath, cityname='defaultcity'):\n",
    "    # Step 1: Load the data\n",
    "    data = pd.read_csv(filepath)\n",
    "\n",
    "    # Dictionary to hold station name as key and its properties as value\n",
    "    station_properties = {}\n",
    "\n",
    "    # Step 2: Iterate through each unique station\n",
    "    for station_name in data['STATION'].unique():\n",
    "        station_data = data[data['STATION'] == station_name]\n",
    "\n",
    "        # Step 3: Check if the station has complete data\n",
    "        unique_dates = len(station_data['DATE'].unique())\n",
    "        if unique_dates >= 5000:\n",
    "            \n",
    "            # Step 4: Count missing values in \"PRCP\" column for that station\n",
    "            missing_prcp = station_data['PRCP'].isnull().sum()\n",
    "            \n",
    "            station_properties[station_name] = {\n",
    "                'missing_count': missing_prcp,\n",
    "                'unique_dates': unique_dates\n",
    "            }\n",
    "\n",
    "    # Step 5: Sort stations based on their missing values and get the top 2 with least missing values\n",
    "    sorted_stations = sorted(station_properties.items(), key=lambda x: x[1]['missing_count'])[:2]\n",
    "\n",
    "    # Extracting data of the top 2 stations from the original dataset\n",
    "    selected_data_1 = data[data['STATION'] == sorted_stations[0][0]]\n",
    "    selected_data_2 = data[data['STATION'] == sorted_stations[1][0]]\n",
    "\n",
    "    # Concatenate the two datasets\n",
    "    combined_data = pd.concat([selected_data_1, selected_data_2], axis=0)\n",
    "\n",
    "    # Save the concatenated data to a CSV file\n",
    "    combined_data.to_csv(f\"{cityname}_2_selected_Stations.csv\", index=False)\n",
    "    \n",
    "    # Return the names and properties of the top 2 stations for reference\n",
    "    result = [(item[0], item[1]['missing_count'], item[1]['unique_dates']) for item in sorted_stations]\n",
    "    return result\n",
    "\n",
    "# Sample usage\n",
    "top_2_stations = process_data('seattle_PRCP.csv', cityname='Seattle')\n",
    "print(top_2_stations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before filling: 3\n",
      "Number of missing values after filling: 0\n"
     ]
    }
   ],
   "source": [
    "def fill_and_save_missing_values(filename, cityname='defaultcity'):\n",
    "    # Step 1: Load the data\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    # Step 2: Count the number of missing values in \"PRCP\" column before filling\n",
    "    missing_before = data['PRCP'].isnull().sum()\n",
    "\n",
    "    # Step 3: Fill missing values in \"PRCP\" column using interpolation method\n",
    "    data['PRCP'] = data['PRCP'].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # Step 4: Count the number of missing values in \"PRCP\" column after filling\n",
    "    missing_after = data['PRCP'].isnull().sum()\n",
    "\n",
    "    # Step 5: Save the filled data back to the same CSV file\n",
    "    data.to_csv(f\"{cityname}_2_selected_Stations.csv\", index=False)\n",
    "\n",
    "    # Step 6: Return the counts of missing values before and after filling\n",
    "    return missing_before, missing_after\n",
    "\n",
    "# Sample usage\n",
    "missing_before, missing_after = fill_and_save_missing_values('Seattle_2_selected_Stations.csv', cityname='Seattle')\n",
    "print(f\"Number of missing values before filling: {missing_before}\")\n",
    "print(f\"Number of missing values after filling: {missing_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10228, 8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Seattle_2_selected_Stations.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station_name: USC00458508\n",
      "2003   0.27   97.03\n",
      "2004   0.30   110.52\n",
      "2005   0.24   86.64\n",
      "2006   0.29   106.17\n",
      "2007   0.28   100.49\n",
      "2008   0.30   110.65\n",
      "2009   0.28   100.93\n",
      "2010   0.31   112.01\n",
      "2011   0.34   123.44\n",
      "2012   0.33   121.21\n",
      "2013   0.29   105.13\n",
      "2014   0.33   120.26\n",
      "2015   0.27   97.12\n",
      "2016   0.29   107.69\n",
      "Overall   0.29   1499.28\n",
      "\n",
      "Station_name: USW00024233\n",
      "2003   0.11   41.78\n",
      "2004   0.08   31.10\n",
      "2005   0.10   35.44\n",
      "2006   0.13   48.42\n",
      "2007   0.11   38.95\n",
      "2008   0.08   30.73\n",
      "2009   0.11   38.44\n",
      "2010   0.13   46.99\n",
      "2011   0.10   36.39\n",
      "2012   0.13   48.26\n",
      "2013   0.09   32.56\n",
      "2014   0.13   48.50\n",
      "2015   0.12   44.83\n",
      "2016   0.12   45.18\n",
      "Overall   0.11   567.57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_annual_metrics(filename):\n",
    "    # Step 1: Load the data\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    # Step 2: Convert 'DATE' column to datetime format\n",
    "    data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "\n",
    "    # Step 3: Extract year from 'DATE' column\n",
    "    data['YEAR'] = data['DATE'].dt.year\n",
    "\n",
    "    # Step 4: Group by 'STATION' and 'YEAR', then compute mean and sum of 'PRDE'\n",
    "    grouped = data.groupby(['STATION', 'YEAR'])['PRCP'].agg(['mean', 'sum'])\n",
    "\n",
    "    # Step 5: Display the results in the desired format\n",
    "    for station_name, sub_data in grouped.groupby(level=0):\n",
    "        print(f\"Station_name: {station_name}\")\n",
    "        overall_mean = 0\n",
    "        overall_sum = 0\n",
    "        count_years = 0\n",
    "        for year, row in sub_data.iterrows():\n",
    "            print(f\"{year[1]}   {row['mean']:.2f}   {row['sum']:.2f}\")\n",
    "            overall_mean += row['mean']\n",
    "            overall_sum += row['sum']\n",
    "            count_years += 1\n",
    "        print(f\"Overall   {overall_mean/count_years:.2f}   {overall_sum:.2f}\")\n",
    "        print()\n",
    "\n",
    "# Sample usage\n",
    "compute_annual_metrics('Seattle_2_selected_Stations.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
